{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62070c94",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Final performances for reference:\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "155ec8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Negative F1</th>\n",
       "      <th>Neutral F1</th>\n",
       "      <th>Positive F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.682961</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.613208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.689099</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.590244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.721451</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.797590</td>\n",
       "      <td>0.625298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.752987</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.832947</td>\n",
       "      <td>0.690722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.794777</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.844933</td>\n",
       "      <td>0.741419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT</td>\n",
       "      <td>0.803895</td>\n",
       "      <td>0.757282</td>\n",
       "      <td>0.882286</td>\n",
       "      <td>0.772118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Macro F1  Negative F1  Neutral F1  Positive F1\n",
       "0   MLP  0.682961     0.637168    0.798507     0.613208\n",
       "1   RNN  0.689099     0.694444    0.782609     0.590244\n",
       "2  LSTM  0.721451     0.741463    0.797590     0.625298\n",
       "3   GRU  0.752987     0.735294    0.832947     0.690722\n",
       "4  BERT  0.794777     0.797980    0.844933     0.741419\n",
       "5   GPT  0.803895     0.757282    0.882286     0.772118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_scores(cm):\n",
    "    cm = np.array(cm)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for true_class in range(cm.shape[0]):\n",
    "        for pred_class in range(cm.shape[1]):\n",
    "            count = cm[true_class, pred_class]\n",
    "            y_true += [true_class] * count\n",
    "            y_pred += [pred_class] * count\n",
    "    \n",
    "    # Macro F1 score\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Per-class F1 scores\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "    \n",
    "    return {\n",
    "        'Macro F1': macro_f1,\n",
    "        'Negative F1': f1_per_class[0],\n",
    "        'Neutral F1': f1_per_class[1],\n",
    "        'Positive F1': f1_per_class[2]\n",
    "    }\n",
    "\n",
    "# Copied from the confusion matrices from the outputs folder\n",
    "mlp_cm = [[72,8,11], [32,321,79], [31,43,130]]\n",
    "rnn_cm = [[75,10,6], [29,324,79], [21,62,121]]\n",
    "lstm_cm = [[76,9,6], [23,331,78], [15,58,131]]\n",
    "gru_cm = [[75,13,3], [26,359,47], [12,58,134]]\n",
    "bert_cm = [[79,8,4], [19,346,67], [9,33,162]]\n",
    "gpt_cm = [[78,11,2], [23,386,23], [14,46,144]]\n",
    "\n",
    "models = {\n",
    "    'MLP': mlp_cm,\n",
    "    'RNN': rnn_cm,\n",
    "    'LSTM': lstm_cm,\n",
    "    'GRU': gru_cm,\n",
    "    'BERT': bert_cm,\n",
    "    'GPT': gpt_cm\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, cm in models.items():\n",
    "    f1_scores = compute_f1_scores(cm)\n",
    "    f1_scores['Model'] = model\n",
    "    results.append(f1_scores)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df[['Model', 'Macro F1', 'Negative F1', 'Neutral F1', 'Positive F1']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Reformat df for plotting\n",
    "df_long = df.melt(\n",
    "    id_vars='Model', \n",
    "    value_vars=['Macro F1', 'Negative F1', 'Neutral F1', 'Positive F1'],\n",
    "    var_name='Metric',\n",
    "    value_name='F1 Score'\n",
    ")\n",
    "\n",
    "sns.barplot(df_long, x='Metric', y='F1 Score', hue='Model', palette='viridis')\n",
    "\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.2, 1.02))\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703284e",
   "metadata": {},
   "source": [
    "## 1. Training Dynamics\n",
    "*Focus on your MLP and LSTM implementations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24302e4",
   "metadata": {},
   "source": [
    "- Did your models show signs of **overfitting** or **underfitting**? What architectural or training changes could address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eaa929",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "MLP Learning Curves:\n",
    "\n",
    "<img src=\"../../outputs/mlp_f1_learning_curves.png\" width=\"800\"/>\n",
    "\n",
    "LSTM Learning Curves:\n",
    "\n",
    "<img src=\"../../outputs/lstm_f1_learning_curves.png\" width=\"800\"/>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d27ff7",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "For my MLP implementation, there are signs of very mild overfitting. This is mainly based on how the learning curves (loss, F1, and accuracy) for the training and validation sets slow down and plateau in later epochs, ultimately intersecting and leaving the validation curves performing slightly worse than the training curves (i.e. val loss curve starts being slightly higher than the train loss, and val F1/accuracy curves start being slightly lower than the train F1/accuracy curves). Additionally, the final train F1 for epoch 30 (0.7183) is slightly higher than the test F1 (0.6830), further suggesting mild overfitting. \n",
    "\n",
    "For my LSTM implementation, there are signs of some overfitting. This is based on how each metric's learning curve plots shows the validation curve lagging behind the train curve starting around epochs 15-20, with the loss curves showing the most drastic difference (ending with train loss of 0.4772 but val loss of 0.6808). Additionally, for each metric, at the last epoch, the train curve looks like it could continue improving in performance (i.e. train loss would continue decreasing, train F1/accuracy continuing to increase), whereas the val curves show signs of plateauing. Indeed, the final train F1 for epoch 30 (0.7875) is higher than the test F1 (0.7215). Altogether, my LSTM model shows signs of overfitting, to an extent greater than the MLP model did.\n",
    "\n",
    "Changes that could address both models' overfitting include early stopping when the validation loss stops improving, stronger regularization (e.g. strengthening the weight decay in the optimizer, increasing dropout), and reducing model capacity (parameters) by reducing the number and/or the size of hidden layers. These suggestions would be more applicable for the LSTM model that shows more signs of overfitting.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158df2c",
   "metadata": {},
   "source": [
    "- How did using **class weights** affect training stability and final performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00cce8",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Mechanically, class weights are used to address class imbalance in the dataset by scaling the loss contribution of samples based on their true class. More specifically, the loss contribution of samples from minority classes are scaled up relative to samples from majority classes, meaning that misclassifications of samples of less represented classes are penalized more heavily than samples of more represented classes. \n",
    "\n",
    "Class weights can negatively affect training stability if the weighting is too strong, leading to larger gradient updates (i.e. overcorrections) for errors in classifying minority samples. However, class weights still generally improve final performance as it prevents models from making predictions biased to the majority class (e.g. without class weights, always guessing the majority class would technically give decent performance as that majority would be classified correctly, but such a model would be extremely biased and not particularly useful). For the dataset used in this assignment, class weights to handle class imbalance would be generally beneficial to final performances as the dataset has a significantly imbalanced class distribution.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40272eab",
   "metadata": {},
   "source": [
    "## 2. Model Performance and Error Analysis\n",
    "*Focus on your MLP and LSTM implementations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55407ecb",
   "metadata": {},
   "source": [
    "- Which of your two models **generalized better** to the test set? Provide evidence from your metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3b45f",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Note that the gap between the final train F1 and test F1 scores is larger for the LSTM model (0.7875 and 0.7215 respectively, giving a gap of about 0.06) than the MLP model (0.7183 and 0.6830 respectively, giving a gap of about 0.03). This is likely from how, as mentioned before, the LSTM model shows more signs of potential overfitting than the MLP model. However, the LSTM model ultimately still has a higher test F1 score than the MLP model (0.7215 > 0.6830). This means that the LSTM model performs better on the same test set given the same training set as the MLP model. Since generalizability is defined as how well a model performs on a test set of data that the model has not seen in training, the higher test F1 score for the LSTM model can be interpreted as it being better at generalizing than the MLP model. \n",
    "\n",
    "This is despite the fact that the LSTM model shows greater signs of overfitting, which should be irrelevant in terms of gauging its generalizability as overfitting is essentially how much the model is merely memorizing the training set. This means overfitting is connected to but still ultimately distinct from generalizability.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d0514",
   "metadata": {},
   "source": [
    "- Which **sentiment class** was most frequently misclassified? Propose reasons for this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdd34d",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Recall essentially measures how many actual positives were correctly classified (defined as $\\frac{TP}{TP+FN}$), so it is an appropriate measure of the rate of misclassification. Thus, the positive (2) sentiment class was most frequently misclassified as its recall is the lowest for both the MLP (0.6373) and LSTM (0.6422) models. \n",
    "\n",
    "A potential reason for this pattern is that positive sentiments in financial language could be mild and thus harder to distinguish from a neutral sentiment. Furthermore, in contrast, negative sentiments could have stronger language more often. This is supported by the confusion matrices for the MLP and LSTM models as both models show that the bulk of misclassifications of positives were actual positives that were predicted as neutral, and most of the misclassifications of neutrals were actual neutrals that were predicted as positive. Additionally, the recall for the negative class is the best of the three classes for both models. In other words, both models seem to struggle the most with differentiating neutral from positive sentiments, while negative sentiments appear relatively easy to classify. \n",
    "\n",
    "This struggle to differentiate neutral sentiments from positive sentiments is reflected more in the models' recalls for the positive sentiment than the neutral sentiment because the neutral class still has a much greater proportion of samples overall that were correctly predicted as neutral. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a38b09",
   "metadata": {},
   "source": [
    "## 3. Cross-Model Comparison\n",
    "*Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f450d2a",
   "metadata": {},
   "source": [
    "- How did **mean-pooled FastText embeddings** limit the MLP compared to sequence-based models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2120865",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "The biggest limitation of using mean-pooled FastText embeddings compared to sequence-based models is that mean-pooling removes sequential structure such as word order. Sentences with similar tokens but in different orders would have nearly identical sentence embeddings under the mean-pooled approach, even if those different word orders would mean those sentences have different sentiments since sentiments depend on more than merely the words that are used. Similarly, mean-pooled FastText embeddings would also be unable to capture context as well as sequence-based models as context depends on how words are strung together in order. Thus, there are significant aspects of sentence meanings that the MLP model would not be able to capture as well as sequence-based models.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0ffa3",
   "metadata": {},
   "source": [
    "- What advantage did the LSTMâ€™s **sequential processing** provide over the MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf6910",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Although the LSTM model also uses the same FastText embeddings as the MLP model, one advantage provided by its sequential processing is that it can preserve word order due to how it maintains representations of sentences in hidden states that get updated with each consecutive token in the sentence. This process allows the LSTM model to also handle long-range dependencies across tokens, which is important when earlier tokens affect the meanings of later tokens, such as in sentences with negation (\"note\") or contrast (\"but\"). Thus, through the sequential processing, the LSTM model has the advantage of capturing context and sequential structure better than the MLP model since the MLP model essentially disregards word order.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931af4",
   "metadata": {},
   "source": [
    "- Did **fine-tuned LLMs** (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901f518",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "Yes, the fine-tuned LLMs outperformed the classical approaches given by how the top two performing models out of the six total were the GPT and BERT models (test macro F1 scores of 0.804 and 0.795 respectively, compared to the next best test macro F1 score of 0.753 for the GRU model). \n",
    "\n",
    "For all models, there was some pretraining, but the specifics differed. The MLP and LSTM models used static FastText embeddings that were pretrained on Wikipedia and news articles. This comes with the limitation of each word having fixed embeddings, making it harder for the models to understand context and sentence-level meanings beyond mere lexical knowledge. The issue of poor contextual representations is especially relevant for the MLP model, which lacks the sequential processing in the LSTM model that allows the LSTM model to gain a bit of context in its sentence representations. \n",
    "\n",
    "In contrast, the RNN and GRU models used a pretrained sentence encoder (MiniLM), which is transformer-based (and thus has layers of self-attention), allowing for better capture of context as a word's embedding would depend on surrounding tokens. However, like the MLP and LSTM models, the RNN and GRU approaches still only use fixed embeddings and lack fine-tuning, meaning that the representations the downstream classifiers train on are fixed.\n",
    "\n",
    "Compared to the other four models, the BERT and GPT models are pretrained on massive corpuses, and they also involve fine-tuning. This means that not only do they have strong base embedding models, nut they are also capable of updating (\"fine-tuning\") the embeddings used for the classifier according to the classification task itself. This allows them to better capture contexts in ways that are more relevant for sentiment classification of finance-related sentences. In other words, the fine-tuned LLMs have the most robust pretrained models with dynamic contextual representations that are adjusted for the specific sentiment classification task at hand. Given that, it is not surprising that the BERT and GPT models perform significantly better than the other classical models.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12a5c1",
   "metadata": {},
   "source": [
    "- **Rank all six models** by test performance. What architectural or representational factors explain the ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cae666",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "From best to worst test macro F1 scores, the models are: GPT (0.804) > BERT (0.795) > GRU (0.753) > LSTM (0.721) > RNN (0.689) > MLP (0.683).\n",
    "\n",
    "Architectural and/or representational factors explaining the ranking include: \n",
    "\n",
    "- scale of the pretrained embeddings (i.e. how big the pretraining corpuses were, how much knowledge the model had to provide the classifier)\n",
    "  - BERT and GPT had the largest corpuses, which is part of why they perform better than the other models.\n",
    "- ability to capture long-range dependencies\n",
    "  - MLP has no sequential processing at all, which leads to it being the worst performer. The RNN model has sequential processing, but it is shallow (only 1 recurrent layer) and lacks gating (and thus has worse long-term memory). The LSTM and GRU models have sequential processing, but also gated memory, making it better with long-range dependencies. The BERT and GPT models have global self-attention, which is even better than sequential processing at things like long-range dependencies.\n",
    "- presence of fine-tuning (as opposed to fixed representations)\n",
    "  - MLP, LSTM, RNN, and GRU models all involve some form of fixed embeddings (MLP and LSTM used static FastText embeddings, RNN and GRU used fixed MiniLM sentence embeddings). BERT and GPT have fine-tuning, which contributes to their better performance.\n",
    "- architectural depth (deeper roughly correlates with performance)\n",
    "  - The MLP and RNN architectures are relatively shallow. LSTM and GRU models have gating which adds complexity but still are shallow in terms of how many layers are stacked. BERT and GPT contain multiple stacked transformer layers.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "I used ChatGPT for concept explanation (e.g. fine-tuning, training stability, self-attention, gating) and debugging (figuring out why my LSTM model was not learning due to issues with padding). I verified things by doing my own Google searches and finding non-AI sources to confirm my understanding. I also referenced the lecture slides to understand what to expect in terms of how the models rank in terms of performances.\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat359-su25-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
